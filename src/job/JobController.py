from src.utils.GeminiService import GeminiService
from src.utils.PineconeService import PineconeService
from src.utils.RecommendService import RecommendService
from src.utils.Timer import Timer
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
import os
from dotenv import load_dotenv

load_dotenv()

class JobController:

    def __init__(self, debug=False):
        self.geminiService = GeminiService()
        self.pineconeService = PineconeService()
        self.recommendService = RecommendService()
        self.debug = debug
        
        # Kh·ªüi t·∫°o LLM
        self.llmModel = "models/gemini-2.5-flash"
        self.llm = ChatGoogleGenerativeAI(
            model=self.llmModel, 
            temperature=0,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
        
        # Template cho c√¢u tr·∫£ l·ªùi
        self.promptTemplate = """
            B·∫°n l√† tr·ª£ l√Ω AI c·ªßa ·ª©ng d·ª•ng GoodJob - gi√∫p ng∆∞·ªùi d√πng t√¨m c√¥ng vi·ªác ph√π h·ª£p.
            
            QUY T·∫ÆC:
            - Tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT t·ª± nhi√™n, th√¢n thi·ªán
            - Ch·ªçn T·ªêI ƒêA 3 c√¥ng vi·ªác PH√ô H·ª¢P NH·∫§T t·ª´ danh s√°ch
            - C√≥ th·ªÉ ch·ªçn 1, 2 ho·∫∑c 3 c√¥ng vi·ªác tu·ª≥ ƒë·ªô ph√π h·ª£p
            - N√™u r√µ: lo·∫°i d·ªãch v·ª•, gi√°, th·ªùi gian, ƒë·ªãa ƒëi·ªÉm
            - S·∫Øp x·∫øp theo ƒë·ªô ph√π h·ª£p (c√¥ng vi·ªác ƒë·∫ßu ti√™n l√† ph√π h·ª£p nh·∫•t)
            - B·∫ÆT BU·ªòC: Khi gi·ªõi thi·ªáu m·ªói c√¥ng vi·ªác, PH·∫¢I ghi r√µ [JobID: XXX] ·ªü ƒë·∫ßu m·ªói c√¥ng vi·ªác
            - N·∫øu kh√¥ng c√≥ c√¥ng vi·ªác ph√π h·ª£p, tr·∫£ l·ªùi l·ªãch s·ª±
            
            C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}
            
            Danh s√°ch c√¥ng vi·ªác t√¨m ƒë∆∞·ª£c (t·ªëi ƒëa 3):
            {jobs_context}
            
            H√£y ch·ªçn v√† gi·ªõi thi·ªáu c√°c c√¥ng vi·ªác PH√ô H·ª¢P NH·∫§T (1-3 c√¥ng vi·ªác) m·ªôt c√°ch t·ª± nhi√™n.
            QUAN TR·ªåNG: M·ªói c√¥ng vi·ªác PH·∫¢I b·∫Øt ƒë·∫ßu b·∫±ng [JobID: XXX]:
        """
        
        self.customPrompt = PromptTemplate(
            input_variables=["query", "jobs_context"],
            template=self.promptTemplate
        )

    # --------------------------------------------------------------

    def _format_jobs_context(self, jobs):
        """Format danh s√°ch jobs th√†nh context cho LLM"""
        if not jobs:
            return "Kh√¥ng t√¨m th·∫•y c√¥ng vi·ªác n√†o."
        
        context_parts = []
        for idx, job in enumerate(jobs, 1):
            service_type_map = {
                "CLEANING": "D·ªçn d·∫πp v·ªá sinh",
                "HEALTHCARE": "ChƒÉm s√≥c s·ª©c kh·ªèe",
                "MAINTENANCE": "B·∫£o tr√¨ thi·∫øt b·ªã"
            }
            
            service_name = service_type_map.get(job.get("serviceType", ""), job.get("serviceType", ""))
            price = job.get("price", "Ch∆∞a c√≥")
            location = job.get("location", "Ch∆∞a r√µ")
            start_time = job.get("startTime", "Ch∆∞a r√µ")
            list_days = job.get("listDays", [])
            days_text = ", ".join(list_days) if list_days else "Ch∆∞a r√µ"
            job_id = job.get("jobID", "")
            
            job_text = f"""
C√¥ng vi·ªác {idx} [JobID: {job_id}]:
- Lo·∫°i d·ªãch v·ª•: {service_name}
- Gi√°: {price} VNƒê
- ƒê·ªãa ƒëi·ªÉm: {location}
- Th·ªùi gian b·∫Øt ƒë·∫ßu: {start_time}
- C√°c ng√†y l√†m vi·ªác: {days_text}
"""
            context_parts.append(job_text.strip())
        
        return "\n\n".join(context_parts)

    # --------------------------------------------------------------

    def _extract_job_ids_from_answer(self, answer):
        """Tr√≠ch xu·∫•t c√°c JobID m√† LLM ƒë√£ ƒë·ªÅ c·∫≠p trong c√¢u tr·∫£ l·ªùi"""
        import re
        # T√¨m t·∫•t c·∫£ pattern [JobID: XXX] ho·∫∑c JobID: XXX
        pattern = r'\[?JobID:\s*([^\]]+)\]?'
        matches = re.findall(pattern, answer, re.IGNORECASE)
        # Clean v√† return unique job IDs
        job_ids = [match.strip() for match in matches]
        return list(set(job_ids))  # Remove duplicates

    # --------------------------------------------------------------

    def _send_to_llm(self, query, jobs):
        """G·ª≠i context v√† query t·ªõi LLM ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n"""
        jobs_context = self._format_jobs_context(jobs)
        prompt = self.customPrompt.format(query=query, jobs_context=jobs_context)
        response = self.llm.invoke(prompt)
        return response.content

    # --------------------------------------------------------------

    def search(self, query: str, reference: dict):
        total_timer = Timer()

        # 1. T·∫°o vector
        step1_timer = Timer()
        # Chu·∫©n ho√° location t·ª´ reference: c√≥ th·ªÉ l√† dict, string, ho·∫∑c thi·∫øu
        raw_location = reference.get("location") if isinstance(reference, dict) else None
        if isinstance(raw_location, dict):
            location_text = raw_location.get("name") or raw_location.get("address") or raw_location.get("formattedAddress") or ""
        elif isinstance(raw_location, str):
            location_text = raw_location
        else:
            location_text = ""

        query_vector_text = f"{query}. T√¥i ·ªü ƒë·ªãa ch·ªâ {location_text}" if location_text else query
        embed = self.geminiService.gemini_get_embedding(query_vector_text)

        if embed is None:
            return {"success": False, "error": "Embedding l·ªói"}

        if self.debug:
            print(f"[DEBUG] üß† Th·ªùi gian t·∫°o embedding: {step1_timer.elapsed_ms():.2f} ms\n")

        # 2. Query Pinecone
        step2_timer = Timer()
        pinecone_result = self.pineconeService.pinecone_search_data(embed, query)

        if not pinecone_result.get("success"):
            return pinecone_result

        jobs = pinecone_result["data"]

        if self.debug:
            print(f"[DEBUG] üß≤ Th·ªùi gian Pinecone query: {step2_timer.elapsed_ms():.2f} ms\n")

        # 3. Recommend - l·∫•y top 3 jobs
        step3_timer = Timer()
        top_jobs = self.recommendService.recommendJob(reference, jobs, top_k=3)

        if self.debug:
            print(f"[DEBUG] üéØ Th·ªùi gian Recommend: {step3_timer.elapsed_ms():.2f} ms\n")

        # 4. T·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n b·∫±ng LLM
        step4_timer = Timer()
        answer = self._send_to_llm(query, top_jobs)
        
        if self.debug:
            print(f"[DEBUG] ü§ñ Th·ªùi gian LLM t·∫°o c√¢u tr·∫£ l·ªùi: {step4_timer.elapsed_ms():.2f} ms\n")

        # 5. L·ªçc jobs theo nh·ªØng job m√† LLM ƒë√£ ƒë·ªÅ c·∫≠p
        mentioned_job_ids = self._extract_job_ids_from_answer(answer)
        filtered_jobs = [job for job in top_jobs if job.get("jobID") in mentioned_job_ids]
        
        # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JobID, fallback v·ªÅ top_jobs
        if not filtered_jobs:
            filtered_jobs = top_jobs
        
        if self.debug:
            print(f"[DEBUG] üìã Jobs ƒë∆∞·ª£c LLM ch·ªçn: {mentioned_job_ids}")
            print(f"[DEBUG] üìä S·ªë l∆∞·ª£ng jobs tr·∫£ v·ªÅ: {len(filtered_jobs)}\n")

        # 6. T·ªïng
        if self.debug:
            print(f"[DEBUG] ‚è± T·ªïng th·ªùi gian x·ª≠ l√Ω: {total_timer.elapsed_ms():.2f} ms\n")

        print(f"[Job] Tr·∫£ l·ªùi: {answer}")
        print(f"[Job] S·ªë l∆∞·ª£ng jobs: {len(filtered_jobs)}")
        print("=================================================================\n")

        return {
            "success": True,
            "message": "Th√†nh c√¥ng",
            "type": "Job",
            "data": {
                "answer": answer,        # C√¢u tr·∫£ l·ªùi t·ª± nhi√™n t·ª´ LLM
                "jobs": filtered_jobs    # Ch·ªâ tr·∫£ v·ªÅ jobs m√† LLM ƒë√£ gi·ªõi thi·ªáu
            }
        }
